{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "01cad8b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:57: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "<>:57: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "C:\\Users\\13269032233\\AppData\\Local\\Temp\\ipykernel_35468\\1190749152.py:57: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
      "  filtered_data = [col for col in transposed_data if not all(cell is '' for cell in col)]\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "import pdfplumber\n",
    "import re\n",
    "from collections import defaultdict\n",
    "import json\n",
    "\n",
    "class PDFProcessor:\n",
    "    def __init__(self, filepath):\n",
    "        self.filepath = filepath\n",
    "        self.pdf = pdfplumber.open(filepath)\n",
    "        self.all_text = defaultdict(dict)\n",
    "        self.allrow = 0\n",
    "        self.last_num = 0\n",
    "\n",
    "    def check_lines(self, page, top, buttom):\n",
    "        lines = page.extract_words()[::]\n",
    "        text = ''\n",
    "        last_top = 0\n",
    "        last_check = 0\n",
    "        for l in range(len(lines)):\n",
    "            each_line = lines[l]\n",
    "            check_re = '(?:。|；|单位：元|单位：万元|币种：人民币|\\d|报告(?:全文)?(?:（修订版）|（修订稿）|（更正后）)?)$'\n",
    "            if top == '' and buttom == '':\n",
    "                if abs(last_top - each_line['top']) <= 2:\n",
    "                    text = text + each_line['text']\n",
    "                elif last_check > 0 and (page.height * 0.9 - each_line['top']) > 0 and not re.search(check_re, text):\n",
    "\n",
    "                    text = text + each_line['text']\n",
    "                else:\n",
    "                    text = text + '\\n' + each_line['text']\n",
    "            elif top == '':\n",
    "                if each_line['top'] > buttom:\n",
    "                    if abs(last_top - each_line['top']) <= 2:\n",
    "                        text = text + each_line['text']\n",
    "                    elif last_check > 0 and (page.height * 0.85 - each_line['top']) > 0 and not re.search(check_re,\n",
    "                                                                                                          text):\n",
    "                        text = text + each_line['text']\n",
    "                    else:\n",
    "                        text = text + '\\n' + each_line['text']\n",
    "            else:\n",
    "                if each_line['top'] < top and each_line['top'] > buttom:\n",
    "                    if abs(last_top - each_line['top']) <= 2:\n",
    "                        text = text + each_line['text']\n",
    "                    elif last_check > 0 and (page.height * 0.85 - each_line['top']) > 0 and not re.search(check_re,\n",
    "                                                                                                          text):\n",
    "                        text = text + each_line['text']\n",
    "                    else:\n",
    "                        text = text + '\\n' + each_line['text']\n",
    "            last_top = each_line['top']\n",
    "            last_check = each_line['x1'] - page.width * 0.85\n",
    "\n",
    "        return text\n",
    "\n",
    "    def drop_empty_cols(self, data):\n",
    "        # 删除所有列为空数据的列\n",
    "        transposed_data = list(map(list, zip(*data)))\n",
    "        filtered_data = [col for col in transposed_data if not all(cell is '' for cell in col)]\n",
    "        result = list(map(list, zip(*filtered_data)))\n",
    "        return result\n",
    "\n",
    "    @staticmethod\n",
    "    def keep_visible_lines(obj):\n",
    "        \"\"\"\n",
    "        If the object is a ``rect`` type, keep it only if the lines are visible.\n",
    "\n",
    "        A visible line is the one having ``non_stroking_color`` not null.\n",
    "        \"\"\"\n",
    "        if obj['object_type'] == 'rect':\n",
    "            if obj['non_stroking_color'] is None:\n",
    "                return False\n",
    "            if obj['width'] < 1 and obj['height'] < 1:\n",
    "                return False\n",
    "            # return obj['width'] >= 1 and obj['height'] >= 1 and obj['non_stroking_color'] is not None\n",
    "        if obj['object_type'] == 'char':\n",
    "            return obj['stroking_color'] is not None and obj['non_stroking_color'] is not None\n",
    "        return True\n",
    "\n",
    "    def extract_text_and_tables(self, page):\n",
    "        buttom = 0\n",
    "        page = page.filter(self.keep_visible_lines)\n",
    "        tables = page.find_tables()\n",
    "        if len(tables) >= 1:\n",
    "            count = len(tables)\n",
    "            for table in tables:\n",
    "                if table.bbox[3] < buttom:\n",
    "                    pass\n",
    "                else:\n",
    "                    count -= 1\n",
    "                    top = table.bbox[1]\n",
    "                    text = self.check_lines(page, top, buttom)\n",
    "                    text_list = text.split('\\n')\n",
    "                    for _t in range(len(text_list)):\n",
    "                        self.all_text[self.allrow] = {'page': page.page_number, 'allrow': self.allrow,\n",
    "                                                      'type': 'text', 'inside': text_list[_t]}\n",
    "                        self.allrow += 1\n",
    "\n",
    "                    buttom = table.bbox[3]\n",
    "                    new_table = table.extract()\n",
    "                    r_count = 0\n",
    "                    for r in range(len(new_table)):\n",
    "                        row = new_table[r]\n",
    "                        if row[0] is None:\n",
    "                            r_count += 1\n",
    "                            for c in range(len(row)):\n",
    "                                if row[c] is not None and row[c] not in ['', ' ']:\n",
    "                                    if new_table[r - r_count][c] is None:\n",
    "                                        new_table[r - r_count][c] = row[c]\n",
    "                                    else:\n",
    "                                        new_table[r - r_count][c] += row[c]\n",
    "                                    new_table[r][c] = None\n",
    "                        else:\n",
    "                            r_count = 0\n",
    "\n",
    "                    end_table = []\n",
    "                    for row in new_table:\n",
    "                        if row[0] != None:\n",
    "                            cell_list = []\n",
    "                            cell_check = False\n",
    "                            for cell in row:\n",
    "                                if cell != None:\n",
    "                                    cell = cell.replace('\\n', '')\n",
    "                                else:\n",
    "                                    cell = ''\n",
    "                                if cell != '':\n",
    "                                    cell_check = True\n",
    "                                cell_list.append(cell)\n",
    "                            if cell_check == True:\n",
    "                                end_table.append(cell_list)\n",
    "                    end_table = self.drop_empty_cols(end_table)\n",
    "\n",
    "                    for row in end_table:\n",
    "                        self.all_text[self.allrow] = {'page': page.page_number, 'allrow': self.allrow,\n",
    "                                                      'type': 'excel', 'inside': str(row)}\n",
    "                        # self.all_text[self.allrow] = {'page': page.page_number, 'allrow': self.allrow, 'type': 'excel',\n",
    "                        #                               'inside': ' '.join(row)}\n",
    "                        self.allrow += 1\n",
    "\n",
    "                    if count == 0:\n",
    "                        text = self.check_lines(page, '', buttom)\n",
    "                        text_list = text.split('\\n')\n",
    "                        for _t in range(len(text_list)):\n",
    "                            self.all_text[self.allrow] = {'page': page.page_number, 'allrow': self.allrow,\n",
    "                                                          'type': 'text', 'inside': text_list[_t]}\n",
    "                            self.allrow += 1\n",
    "\n",
    "        else:\n",
    "            text = self.check_lines(page, '', '')\n",
    "            text_list = text.split('\\n')\n",
    "            for _t in range(len(text_list)):\n",
    "                self.all_text[self.allrow] = {'page': page.page_number, 'allrow': self.allrow,\n",
    "                                              'type': 'text', 'inside': text_list[_t]}\n",
    "                self.allrow += 1\n",
    "\n",
    "        first_re = '[^计](?:报告(?:全文)?(?:（修订版）|（修订稿）|（更正后）)?)$'\n",
    "        end_re = '^(?:\\d|\\\\|\\/|第|共|页|-|_| ){1,}'\n",
    "        if self.last_num == 0:\n",
    "            try:\n",
    "                first_text = str(self.all_text[1]['inside'])\n",
    "                end_text = str(self.all_text[len(self.all_text) - 1]['inside'])\n",
    "                if re.search(first_re, first_text) and not '[' in end_text:\n",
    "                    self.all_text[1]['type'] = '页眉'\n",
    "                    if re.search(end_re, end_text) and not '[' in end_text:\n",
    "                        self.all_text[len(self.all_text) - 1]['type'] = '页脚'\n",
    "            except:\n",
    "                print(page.page_number)\n",
    "        else:\n",
    "            try:\n",
    "                first_text = str(self.all_text[self.last_num + 2]['inside'])\n",
    "                end_text = str(self.all_text[len(self.all_text) - 1]['inside'])\n",
    "                if re.search(first_re, first_text) and '[' not in end_text:\n",
    "                    self.all_text[self.last_num + 2]['type'] = '页眉'\n",
    "                if re.search(end_re, end_text) and '[' not in end_text:\n",
    "                    self.all_text[len(self.all_text) - 1]['type'] = '页脚'\n",
    "            except:\n",
    "                print(page.page_number)\n",
    "\n",
    "        self.last_num = len(self.all_text) - 1\n",
    "\n",
    "\n",
    "    def process_pdf(self):\n",
    "        for i in range(len(self.pdf.pages)):\n",
    "            self.extract_text_and_tables(self.pdf.pages[i])\n",
    "\n",
    "\n",
    "    def save_all_text(self, path):\n",
    "        with open(path, 'w', encoding='utf-8') as file:\n",
    "            for key in self.all_text.keys():\n",
    "                file.write(json.dumps(self.all_text[key], ensure_ascii=False) + '\\n')\n",
    "\n",
    "\n",
    "def process_all_pdfs_in_folder(folder_path):\n",
    "    file_paths = glob.glob(f'{folder_path}/*')\n",
    "    file_paths = sorted(file_paths, reverse=True)\n",
    "\n",
    "    for file_path in file_paths:\n",
    "        print(file_path)\n",
    "        try:\n",
    "            processor = PDFProcessor(file_path)\n",
    "            processor.process_pdf()\n",
    "            save_path = 'alltxt/' + file_path.split('/')[-1].replace('.pdf', '.txt')\n",
    "            processor.save_all_text(save_path)\n",
    "        except:\n",
    "            print('check')\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "739adb89",
   "metadata": {},
   "outputs": [],
   "source": [
    "if __name__ == '__main__':\n",
    "    pdf_path = r'D:/微信文件/WeChat Files/a35857710/FileStorage/File/2023-11/红土盐田港REIT：红土创新盐田港仓储物流封闭式基础设施证券投资基金招募说明书（更新）2021年第1期(1)/test.pdf'\n",
    "    out_path = r'D:/reits知识库/具体项目/test_pdf2textnew.txt'\n",
    "    processor = PDFProcessor(pdf_path)\n",
    "    processor.process_pdf()\n",
    "    processor.save_all_text(out_path)\n",
    "\n",
    "# folder_path = 'allpdf'\n",
    "# process_all_pdfs_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f20125a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from preprocess import extract_pdf_text, extract_pdf_tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c12d32d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import shutil\n",
    "# import pdfplumber\n",
    "# import camelot\n",
    "from multiprocessing import Pool\n",
    "from loguru import logger\n",
    "# from langchain.document_loaders import UnstructuredPDFLoader\n",
    "# from langchain.document_loaders import PDFPlumberLoader\n",
    "# from langchain.document_loaders import TextLoader\n",
    "# from langchain.text_splitter import CharacterTextSplitter, RecursiveCharacterTextSplitter\n",
    "# from langchain.schema import Document\n",
    "# from langchain.embeddings.huggingface import HuggingFaceEmbeddings\n",
    "# from langchain.vectorstores import FAISS\n",
    "\n",
    "from config import cfg\n",
    "from file import load_pdf_info\n",
    "# from chinese_text_splitter import ChineseTextSplitter\n",
    "# from pdf2txt import PDFProcessor\n",
    "from pdf_util import PdfExtractor\n",
    "from financial_state import (extract_basic_info, extract_employee_info,\n",
    "    extract_cbs_info, extract_cscf_info, extract_cis_info, extract_dev_info, merge_info)\n",
    "\n",
    "\n",
    "def setup_xpdf():\n",
    "    os.chdir(cfg.XPDF_PATH)\n",
    "    cmd = 'chmod +x pdftotext'\n",
    "    os.system(cmd)\n",
    "\n",
    "\n",
    "def extract_pure_content(idx, key, pdf_path):\n",
    "    logger.info('Extract text for {}:{}'.format(idx, key))\n",
    "    save_dir = os.path.join(cfg.DATA_PATH, cfg.PDF_TEXT_DIR)\n",
    "    key_dir = os.path.join(save_dir, key)\n",
    "    if not os.path.exists(key_dir):\n",
    "        os.mkdir(key_dir)\n",
    "    save_path = os.path.join(key_dir, 'pure_content.txt')\n",
    "    if os.path.exists(save_path):\n",
    "        os.remove(save_path)\n",
    "    PdfExtractor(pdf_path).extract_pure_content_and_save(save_path)\n",
    "\n",
    "# def extract_text(idx, key, pdf_path):\n",
    "#     print(idx, key, pdf_path)\n",
    "#     save_dir = os.path.join(cfg.DATA_PATH, __pdf_text_dir__)\n",
    "#     key_dir = os.path.join(save_dir, key)\n",
    "#     if not os.path.exists(key_dir):\n",
    "#         os.mkdir(key_dir)\n",
    "#     save_path = os.path.join(key_dir, 'docs.txt')\n",
    "#     # if os.path.exists(save_path):\n",
    "#     #     return\n",
    "#     # else:\n",
    "#         # os.chdir(__xpdf_path__)\n",
    "#         # cmd = './pdftotext -lineprinter \"{}\" \"{}\"'.format(pdf_path, save_path)\n",
    "#         # print(cmd)\n",
    "#         # os.system(cmd)\n",
    "#     try:\n",
    "#         processor = PDFProcessor(pdf_path)\n",
    "#         processor.process_pdf()\n",
    "#         processor.save_all_text(save_path)\n",
    "#         # PdfExtractor(pdf_path).extract_and_save(save_path)\n",
    "#     except Exception as e:\n",
    "#         print(e, pdf_path)\n",
    "\n",
    "\n",
    "def extract_pdf_text(extract_func=extract_pure_content):\n",
    "    setup_xpdf()\n",
    "\n",
    "    save_dir = os.path.join(cfg.DATA_PATH, cfg.PDF_TEXT_DIR)\n",
    "    if not os.path.exists(save_dir):\n",
    "        os.mkdir(save_dir)\n",
    "\n",
    "    pdf_info = load_pdf_info()\n",
    "\n",
    "    # for i, (k, v) in enumerate(pdf_info.items()):\n",
    "    #     extract_func(i, k, v['pdf_path'])\n",
    "\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.starmap(extract_func, [(i, k, v['pdf_path']) for i, (k, v) in enumerate(pdf_info.items())])\n",
    "\n",
    "\n",
    "def extract_pdf_tables():\n",
    "    pdf_info = load_pdf_info()\n",
    "    pdf_keys = list(pdf_info.keys())\n",
    "\n",
    "    # basic_info\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.map(extract_basic_info, pdf_keys)\n",
    "    merge_info('basic_info')\n",
    "    # # employee_info\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.map(extract_employee_info, pdf_keys)\n",
    "    merge_info('employee_info')\n",
    "    # cbs_info\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.map(extract_cbs_info, pdf_keys)\n",
    "    merge_info('cbs_info')\n",
    "    # cscf_info\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.map(extract_cscf_info, pdf_keys)\n",
    "    merge_info('cscf_info')\n",
    "    # cis_info\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.map(extract_cis_info, pdf_keys)\n",
    "    merge_info('cis_info')\n",
    "    # dev_info\n",
    "    with Pool(processes=cfg.NUM_PROCESSES) as pool:\n",
    "        results = pool.map(extract_dev_info, pdf_keys)\n",
    "    merge_info('dev_info')\n",
    "\n",
    "\n",
    "# def generate_embedding_vector(key, embedding):\n",
    "#     text_path = os.path.join(cfg.DATA_PATH, __pdf_text_dir__, key, 'docs.txt')\n",
    "#     loader = TextLoader(text_path, encoding='utf-8')\n",
    "#     docs = loader.load_and_split(text_splitter=RecursiveCharacterTextSplitter(\n",
    "#         separators=['\\n'], keep_separator=False,\n",
    "#         chunk_size=1024, chunk_overlap=0,\n",
    "#         length_function=len, add_start_index=True))\n",
    "#     # for doc in docs:\n",
    "#     #     print(len(doc.page_content))\n",
    "#     #     print(doc.page_content)\n",
    "#     #     print(doc.metadata)\n",
    "#     #     print('*'*100)\n",
    "#     # exit(0)\n",
    "    \n",
    "#     doc_vecs = FAISS.from_documents(docs, embedding)\n",
    "#     doc_vecs.save_local(os.path.join(cfg.DATA_PATH, __pdf_text_dir__, key, 'doc_vecs'))\n",
    "\n",
    "\n",
    "# def generate_embedding_all():\n",
    "#     os.environ['CUDA_VISIBLE_DEVICES'] = '3'\n",
    "\n",
    "#     # embeddings = None\n",
    "#     connection_error = True\n",
    "#     while connection_error:\n",
    "#         try:\n",
    "#             embeddings = HuggingFaceEmbeddings(model_name='GanymedeNil/text2vec-large-chinese')\n",
    "#             connection_error = False\n",
    "#         except Exception as e:\n",
    "#             print(e)\n",
    "#             continue\n",
    "#     with open(os.path.join(cfg.DATA_PATH, 'pdf_info.json')) as f:\n",
    "#         pdf_info = json.load(f)\n",
    "\n",
    "#     for k, v in pdf_info.items():\n",
    "#         print(k)\n",
    "#         generate_embedding_vector(k, embeddings)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import os\n",
    "    import time\n",
    "    # import ghostscript\n",
    "    os.environ['PATH'] = r'C:\\Program Files\\gs\\gs10.01.2\\bin;' + os.environ['PATH']\n",
    "    # import ctypes\n",
    "    # from ctypes.util import find_library\n",
    "    # lib = find_library(\"\".join((\"gsdll\", str(ctypes.sizeof(ctypes.c_voidp) * 8), \".dll\")))\n",
    "    # print(lib)\n",
    "    # import camelot\n",
    "    # generate_embedding_all()\n",
    "    # extract_text_all(extract_func=extract_pure_content)\n",
    "\n",
    "\n",
    "    # extract_pure_content(0, '2020-03-25__南京钢铁股份有限公司__600282__南钢股份__2019年__年度报告.pdf',\n",
    "    #     '/raidnvme/czc/MODELSCOPE_CACHE_HOME/modelscope/hub/datasets/modelscope/chatglm_llm_fintech_raw_dataset/master/data_files/1106979bbfe796043d45ea0f4831c916802713a7b08a580e98421d91d8ba0eb3')\n",
    "\n",
    "    pdf_path = r'C:\\Users\\CHENZHAOCAI\\Downloads\\test.pdf'\n",
    "    out_path = r'C:\\Users\\CHENZHAOCAI\\Downloads\\test.txt'\n",
    "\n",
    "    # pdf_path = '/raidnvme/czc/MODELSCOPE_CACHE_HOME/modelscope/hub/datasets/modelscope/chatglm_llm_fintech_raw_dataset/master/data_files/011af0d314a605ab3cff699f48af52248d2d9fabe417b811321d11107fa49c97'\n",
    "\n",
    "\n",
    "    # start = time.time()\n",
    "    PdfExtractor(pdf_path).extract_table_of_pages([103])\n",
    "    # PdfExtractor(pdf_path).extract_pure_content_and_save(out_path, True)\n",
    "\n",
    "    # end = time.time()\n",
    "    # print(end - start)\n",
    "\n",
    "    # from file import load_pdf_info, load_pdf_pure_text\n",
    "    # pdf_info = load_pdf_info()\n",
    "\n",
    "    # for k, v in pdf_info.items():\n",
    "    #     # print(k, v['pdf_path'])\n",
    "    #     text_lines = load_pdf_pure_text(k) \n",
    "    #     if len(text_lines) == 0:\n",
    "    #         extract_pure_content(0, k, v['pdf_path'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
